{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api_key = \"Be49L9JHyXCXmNGmUwtX\"\n",
    "quandl.ApiConfig.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/Joe/Documents/Metis/Projects/metis-two-Luther/python-scripts'\n",
    "sys.path.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from luther import nasdaq_companies\n",
    "ticker_dict = nasdaq_companies('abbrev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ticker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(filename, filepath='/Users/Joe/Documents/Metis/Projects/metis-two-Luther/wikipedia-revision-logs-abbrev/'):\n",
    "    \"\"\"\n",
    "    Opens the pickled dataframe stored at the specified location.\n",
    "    ---\n",
    "    IN: string\n",
    "    OUT: pandas dataframe\n",
    "    \"\"\"\n",
    "    with open(filepath + filename, 'rb') as picklefile: \n",
    "        df = pickle.load(picklefile)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = open_file('revision-log-ADBE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>size</th>\n",
       "      <th>size_delta</th>\n",
       "      <th>minor_edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-28 07:10:00</td>\n",
       "      <td>Sleske (talk | contribs)</td>\n",
       "      <td>(48,694 bytes)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-09-28 05:05:00</td>\n",
       "      <td>Agrasen8080 (talk | contribs)</td>\n",
       "      <td>(48,694 bytes)</td>\n",
       "      <td>(0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-09-23 18:51:00</td>\n",
       "      <td>TaerkastUA (talk | contribs)</td>\n",
       "      <td>(48,694 bytes)</td>\n",
       "      <td>(-30)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-21 05:30:00</td>\n",
       "      <td>Jon Kolbert (talk | contribs)</td>\n",
       "      <td>(48,724 bytes)</td>\n",
       "      <td>(-8)</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-20 19:40:00</td>\n",
       "      <td>Software121 (talk | contribs)</td>\n",
       "      <td>(48,732 bytes)</td>\n",
       "      <td>(-1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                           user            size  \\\n",
       "0 2017-09-28 07:10:00       Sleske (talk | contribs)  (48,694 bytes)   \n",
       "1 2017-09-28 05:05:00  Agrasen8080 (talk | contribs)  (48,694 bytes)   \n",
       "2 2017-09-23 18:51:00   TaerkastUA (talk | contribs)  (48,694 bytes)   \n",
       "3 2017-09-21 05:30:00  Jon Kolbert (talk | contribs)  (48,724 bytes)   \n",
       "4 2017-09-20 19:40:00  Software121 (talk | contribs)  (48,732 bytes)   \n",
       "\n",
       "  size_delta minor_edit  \n",
       "0        (0)          m  \n",
       "1        (0)          0  \n",
       "2      (-30)          0  \n",
       "3       (-8)          m  \n",
       "4       (-1)          0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(df, filename, filepath='/Users/Joe/Documents/Metis/Projects/metis-two-Luther/combined-wiki-price-data/'):\n",
    "    \"\"\"\n",
    "    Pickles a dataframe to the specified location.\n",
    "    ---\n",
    "    IN: dataframe, string\n",
    "    OUT: void\n",
    "    \"\"\"    \n",
    "    with open(filepath + filename, 'wb') as picklefile:\n",
    "        pickle.dump(df, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_file_name(abbrev):\n",
    "    \"\"\"\n",
    "    From the stock ticker, return the name of the pickle file corresponding to the scraped wikipedia history\n",
    "    of that company.\n",
    "    ---\n",
    "    IN: string\n",
    "    OUT: string\n",
    "    \"\"\"\n",
    "    filename = 'revision-log-' + abbrev + '.pkl'\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abbrev_from_filename(filename):\n",
    "    \"\"\"\n",
    "    From the filename of the pickled wikipedia dataframe, return the stock ticker.\n",
    "    ---\n",
    "    IN: string\n",
    "    OUT: string\n",
    "    \"\"\"\n",
    "    abbrev = filename.replace('revision-log-', '')\n",
    "    abbrev = abbrev.replace('.pkl', '')\n",
    "    return abbrev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Organize Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_wikipedia_df(filename):\n",
    "    \"\"\"\n",
    "    Loads a saved dataframe of wikipedia revision history and returns a cleaned dataframe\n",
    "    ---\n",
    "    IN: string\n",
    "    OUT: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = open_file(filename)\n",
    "    \n",
    "    df.rename(columns={'date': 'datetime',\n",
    "                       'size': 'size (bytes)'}, inplace=True)\n",
    "    \n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    \n",
    "    df['user'] = df['user'].apply(lambda x: x.split('(')[0])\n",
    "    df['size (bytes)'] = df['size (bytes)'].apply(lambda x: x.replace('(empty)', '0')) # Note: Filling in data\n",
    "    df['size (bytes)'] = df['size (bytes)'].apply(lambda x: x.strip('('))\n",
    "    df['size (bytes)'] = df['size (bytes)'].apply(lambda x: x.replace(' bytes)', ''))\n",
    "    df['size (bytes)'] = df['size (bytes)'].apply(lambda x: x.replace(',', ''))\n",
    "    df['size (bytes)'] = df['size (bytes)'].astype(int)\n",
    "    \n",
    "    df['size_delta'] = df['size_delta'].apply(lambda x: x.replace('(', ''))\n",
    "    df['size_delta'] = df['size_delta'].apply(lambda x: x.replace(')', ''))\n",
    "    df['size_delta'] = df['size_delta'].apply(lambda x: x.replace(',', ''))\n",
    "    df['size_delta'] = df['size_delta'].astype(int)\n",
    "    \n",
    "    df['minor_edit'] = df['minor_edit'].replace('m', 1)\n",
    "    df['edit_count'] = 1\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    return df.groupby('date').sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stock data to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock_data_to_match(abbrev, df_wikipedia):\n",
    "    \"\"\"\n",
    "    Given the stock ticker and corresponding wikipedia history data, \n",
    "    \"\"\"\n",
    "    start = df_wikipedia['date'].min().strftime('%Y-%M-%d')\n",
    "    stop  = df_wikipedia['date'].max().strftime('%Y-%M-%d')\n",
    "    dates = {'gte': start, 'lte': stop}\n",
    "    df_ticker = quandl.get_table('WIKI/PRICES', ticker=abbrev, date=dates)\n",
    "    df_ticker['date'] = pd.to_datetime(df_ticker['date'])\n",
    "    return df_ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def company_history(filename):\n",
    "    abbrev = abbrev_from_filename(filename)\n",
    "    df_wikipedia = format_wikipedia_df(filename)\n",
    "    df_ticker = get_stock_data_to_match(abbrev, df_wikipedia)\n",
    "    df = pd.merge(df_wikipedia, df_ticker, how='inner', on='date')\n",
    "    return df.dropna(), abbrev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this for every company collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait till we finish re-scraping the wikipedia data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath_wikipedia_logs = '/Users/Joe/Documents/Metis/Projects/metis-two-Luther/wikipedia-revision-logs-abbrev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data for AAL\n",
      "Combined data for AAPL\n",
      "Combined data for ADBE\n",
      "Combined data for ADI\n",
      "Combined data for ADP\n",
      "Combined data for ADSK\n",
      "Combined data for AKAM\n",
      "Combined data for ALXN\n",
      "Combined data for AMAT\n",
      "Combined data for AMGN\n",
      "Combined data for AMZN\n",
      "Combined data for ATVI\n",
      "Combined data for AVGO\n",
      "Combined data for BBBY\n",
      "Combined data for BIDU\n",
      "Combined data for BIIB\n",
      "Combined data for BMRN\n",
      "Combined data for CA\n",
      "Combined data for CELG\n",
      "Combined data for CERN\n",
      "Combined data for CHKP\n",
      "Combined data for CHRW\n",
      "Combined data for CHTR\n",
      "Combined data for CMCSA\n",
      "Combined data for COST\n",
      "Combined data for CSCO\n",
      "Combined data for CSX\n",
      "Combined data for CTAS\n",
      "Combined data for CTRP\n",
      "Combined data for CTSH\n",
      "Combined data for CTXS\n",
      "Combined data for DISCA\n",
      "Combined data for DISCK\n",
      "Combined data for DISH\n",
      "Combined data for DLTR\n",
      "Combined data for EA\n",
      "Combined data for EBAY\n",
      "Combined data for EQIX\n",
      "Combined data for ESRX\n",
      "Combined data for EXPD\n",
      "Combined data for EXPE\n",
      "Combined data for FAST\n",
      "Combined data for FB\n",
      "Combined data for FISV\n",
      "Combined data for FLEX\n",
      "Combined data for FLIR\n",
      "Combined data for FOX\n",
      "Combined data for FOXA\n",
      "Combined data for FSLR\n",
      "Combined data for GILD\n",
      "Combined data for GOLD\n",
      "Combined data for GOOG\n",
      "Combined data for GOOGL\n",
      "Combined data for GRMN\n",
      "Combined data for HAS\n",
      "Combined data for HOLX\n",
      "Combined data for HSIC\n",
      "Combined data for ILMN\n",
      "Combined data for INCY\n",
      "Combined data for INTC\n",
      "Combined data for INTU\n",
      "Combined data for ISRG\n",
      "Combined data for JBHT\n",
      "Combined data for JD\n",
      "Combined data for KHC\n",
      "Combined data for KLAC\n",
      "Combined data for LBTYA\n",
      "Combined data for LBTYK\n",
      "Combined data for LRCX\n",
      "Combined data for LVNTA\n",
      "Combined data for MAR\n",
      "Combined data for MAT\n",
      "Combined data for MCHP\n",
      "Combined data for MDLZ\n",
      "Combined data for MNST\n",
      "Combined data for MRVL\n",
      "Combined data for MSFT\n",
      "Combined data for MU\n",
      "Combined data for MXIM\n",
      "Combined data for MYL\n",
      "Combined data for NCLH\n",
      "Combined data for NFLX\n",
      "Combined data for NTAP\n",
      "Combined data for NTES\n",
      "Combined data for NUAN\n",
      "Combined data for NVDA\n",
      "Combined data for NXPI\n",
      "Combined data for ORLY\n",
      "Combined data for PAYX\n",
      "Combined data for PCAR\n",
      "Combined data for PCLN\n",
      "Combined data for PYPL\n",
      "Combined data for QCOM\n",
      "Combined data for QGEN\n",
      "Combined data for QVCA\n",
      "Combined data for REGN\n",
      "Combined data for ROST\n",
      "Combined data for SBUX\n",
      "Combined data for SHPG\n",
      "Combined data for SIRI\n",
      "Combined data for SRCL\n",
      "Combined data for STX\n",
      "Combined data for SWKS\n",
      "Combined data for SYMC\n",
      "Combined data for TMUS\n",
      "Combined data for TRIP\n",
      "Combined data for TSCO\n",
      "Combined data for TSLA\n",
      "Combined data for TXN\n",
      "Combined data for ULTA\n",
      "Combined data for URBN\n",
      "Combined data for VIAB\n",
      "Combined data for VOD\n",
      "Combined data for VRSK\n",
      "Combined data for VRSN\n",
      "Combined data for VRTX\n",
      "Combined data for WBA\n",
      "Combined data for WDC\n",
      "Combined data for WYNN\n",
      "Combined data for XLNX\n",
      "Combined data for XRAY\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(filepath_wikipedia_logs):\n",
    "    if not filename.startswith('.'):\n",
    "        df, abbrev = company_history(filename)\n",
    "        save_file(df, 'wikipedia-and-stock-history-' + abbrev + '.pkl')\n",
    "        print(f'Combined data for {abbrev}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
